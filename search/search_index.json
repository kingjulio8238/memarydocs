{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"memary","text":""},{"location":"#open-source-longterm-memory-for-autonomous-agents","title":"Open-Source Longterm Memory for Autonomous Agents","text":"<p>For full documentation visit our repo and leave a  </p> <p>Why memary</p> <p>Agents use LLMs that are currently constrained to finite context windows. memary overcomes this limitation by allowing your agents to store a large corpus of information in knowledge graphs, infer user knowledge through our memory modules, and only retrieve relevant information for meaningful responses.</p>"},{"location":"#installation","title":"Installation","text":"<p>1st Way</p> <p>Make sure you are running python version &lt;= 3.11.9, then run  <pre><code>pip install memary\n</code></pre></p> <p>2nd Way</p> <p>You can also install memary locally: </p> <p>i. Create a virtual environment with python version set as specified above </p> <p>ii. Install python dependencies:  <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"build/","title":"Build","text":"Model Use <p>memary assumes the local installation method and currently supports any models available through Ollama:</p> <ul> <li>LLM running locally using Ollama (<code>Llama 3 8B/40B</code> as suggested defaults) OR <code>gpt-3.5-turbo</code></li> <li>Vision model running locally using Ollama (<code>LLaVA</code> as suggested default) OR <code>gpt-4-vision-preview</code></li> </ul> <p>memary will default to the locally run models unless explicitly specified. Additionally, memary allows developers to easily switch between downloaded models via Ollama </p>"},{"location":"build/#to-run-memary","title":"To run memary","text":"<ul> <li> <p>(Optional) If running models locally using Ollama, follow this the instructions in this [repo] (https://github.com/ollama/ollama). </p> </li> <li> <p>Ensure that an <code>.env</code> exists with any necessary API keys and Neo4j credentials </p> </li> </ul> .env <pre><code>OPENAI_API_KEY=YOUR_API_KEY\nNEO4J_PW=YOUR_NEO4J_PW\nNEO4J_URL=YOUR_NEO4J_URL\nPERPLEXITY_API_KEY=YOUR_API_KEY\nGOOGLEMAPS_API_KEY=YOUR_API_KEY\nALPHA_VANTAGE_API_KEY=YOUR_API_KEY\n</code></pre> <ul> <li> <p>Update user persona which can be found in <code>streamlit_app/data/user_persona.txt</code> using the user persona template which can be found in <code>streamlit_app/data/user_persona_template.txt</code>. Instructions have been provided for customization - replace the curly brackets with relevant information.</p> </li> <li> <p>(Optional) Update system persona, if needed, which can be found in <code>streamlit_app/data/system_persona.txt</code>. </p> </li> </ul> <p>Lastly Run</p> <pre><code>cd streamlit_app\nstreamlit run app.py\n</code></pre>"},{"location":"build/#usage","title":"Usage","text":"memary_usage<pre><code>from memary.agent.chat_agent import ChatAgent\n\nsystem_persona_txt = \"data/system_persona.txt\"\nuser_persona_txt = \"data/user_persona.txt\"\npast_chat_json = \"data/past_chat.json\"\nmemory_stream_json = \"data/memory_stream.json\"\nentity_knowledge_store_json = \"data/entity_knowledge_store.json\"\nchat_agent = ChatAgent(\n    \"Personal Agent\",\n    memory_stream_json,\n    entity_knowledge_store_json,\n    system_persona_txt,\n    user_persona_txt,\n    past_chat_json,\n)\n</code></pre> <p>Agent Configuration</p> <p>Pass in subset of <code>['search', 'vision', 'locate', 'stocks']</code> as <code>include_from_defaults</code> for different set of default tools upon initialization.</p>"},{"location":"build/#adding-custom-tools","title":"Adding Custom Tools","text":"add_tool<pre><code>def multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two integers and returns the result integer\"\"\"\n    return a * b\n\nchat_agent.add_tool({\"multiply\": multiply})\n</code></pre> <p>ReAct Custom Tools</p> <p>More information about creating custom tools for the LlamaIndex ReAct Agent can be found here. </p>"},{"location":"build/#removing-custom-tools","title":"Removing Custom Tools","text":"remove_tool<pre><code>def multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two integers and returns the result integer\"\"\"\n    return a * b\n\nchat_agent.remove_tool(\"multiply\")\n</code></pre>"},{"location":"concepts/","title":"Main Concepts","text":""},{"location":"concepts/#how-it-works","title":"How it works","text":"<p>The above process includes the routing agent, knoweldge graph and memory modules that are all integrated into the <code>ChatAgent</code> class located in the <code>src/agent</code> directory.</p> <p>Note</p> <p>Raw source code for these components can also be found in their respective directories including benchmarks, notebooks, and updates.</p>"},{"location":"concepts/#detailed-component-breakdown","title":"Detailed Component Breakdown","text":""},{"location":"concepts/#routing-agent","title":"Routing Agent","text":"<p>To provide developers, who don't have existing agents, access to memary we setup a simple agent implementation. We use the ReAct agent to plan and execute a query given the tools provided. </p> <p>While we didn't emphasize equipping the agent with many tools, the search tool is crucial to retrieve information from the knowledge graph. This tool queries the knowledge graph for a response based on existing nodes and executes an external search if no related entities exist. Other default agent tools include computer vision powered by LLaVa and a location tool using geococder and google maps. </p> external_query<pre><code>def external_query(self, query: str):\n    messages_dict = [\n        {\"role\": \"system\", \"content\": \"Be precise and concise.\"},\n        {\"role\": \"user\", \"content\": query},\n    ]\n    messages = [ChatMessage(**msg) for msg in messages_dict]\n    external_response = self.query_llm.chat(messages)\n\n    return str(external_response)\n</code></pre> search<pre><code>def search(self, query: str) -&gt; str:\n    response = self.query_engine.query(query)\n\n    if response.metadata is None:\n        return self.external_query(query)\n    else:\n        return response\n</code></pre> <p>Developers will be able to choose to to intialize memary with the default tools or their own at setup</p> <p>Purpose in larger system</p> <p>Each response from the agent is saved in the knowledge graph. You can view responses from various tools as distinct elements that contribute to the user's knowledge.</p> <p>Future Contributions</p> <ul> <li>Make your own agent and add as many tools as possible! Each tool expands the agent's ability to answer a wide variety of queries.</li> <li>Create an LLM Judge that scores the routing agent and provides feedback.</li> <li>Integrate multiprocessing so that the agent can process multiple sub-queries simultaneously. We have open-sourced the query decomposition and reranking code to help with this!</li> </ul>"},{"location":"concepts/#knowledge-graph","title":"Knowledge Graph","text":""},{"location":"concepts/#knowledge-graphs-llms","title":"Knowledge graphs \u2194 LLMs","text":"<ul> <li>memary uses a Neo4j graph database to store knoweldge.</li> <li>Llama Index was used to add nodes into the graph store based on documents.</li> <li>Perplexity (mistral-7b-instruct model) was used for external queries.</li> </ul>"},{"location":"concepts/#kg-use-cases","title":"KG use cases","text":"<ul> <li>Inject the final agent responses into existing KGs.</li> <li>memary uses a recursive retrieval approach to search the KG, which involves determining what the key entities are in the query, building a subgraph of those entities with a maximum depth of 2 away, and finally using that subgraph to build up the context.</li> <li>When faced with multiple key entities in a query, memary uses multi-hop reasoning to join multiple subgraphs into a larger subgraph to search through.</li> <li>These techniques reduce latency compared to searching the entire knowledge graph at once.</li> </ul> store in KG<pre><code>def query(self, query: str) -&gt; str:\n        # get the response from react agent\n        response = self.routing_agent.chat(query)\n        self.routing_agent.reset()\n        # write response to file for KG writeback\n        with open(\"data/external_response.txt\", \"w\") as f:\n            print(response, file=f)\n        # write back to the KG\n        self.write_back()\n        return response\n</code></pre> recursive retrieval<pre><code>def check_KG(self, query: str) -&gt; bool:\n        \"\"\"Check if the query is in the knowledge graph.\n\n        Args:\n            query (str): query to check in the knowledge graph\n\n        Returns:\n            bool: True if the query is in the knowledge graph, False otherwise\n        \"\"\"\n        response = self.query_engine.query(query)\n\n        if response.metadata is None:\n            return False\n        return generate_string(\n            list(list(response.metadata.values())[0][\"kg_rel_map\"].keys())\n        )\n</code></pre> <p>Purpose in larger system</p> <p>Continuously updates the memory modules with each node insertion.</p> <p>Future Contributions</p> <ul> <li>Expand the graph\u2019s capabilities to support multiple modalities, i.e., images.</li> <li>Graph optimizations to reduce latency of search times.</li> </ul>"},{"location":"concepts/#memory-module","title":"Memory Module","text":"<p>The memory module comprises the Memory Stream and Entity Knowledge Store. The memory module was influenced by the design of K-LaMP proposed by Microsoft Research.</p>"},{"location":"concepts/#memory-stream","title":"Memory Stream","text":"<p>The Memory Stream captures all entities inserted into the KG and their associated timestamps. This stream reflects the breadth of the users' knowledge, i.e., concepts users have had exposure to but no depth of exposure is inferred. - Timeline Analysis: Map out a timeline of interactions, highlighting moments of high engagement or shifts in topic focus. This helps in understanding the evolution of the user's interests over time.</p> add to memory stream<pre><code>def add_memory(self, entities):\n        self.memory.extend([\n            MemoryItem(str(entity),\n                       datetime.now().replace(microsecond=0))\n            for entity in entities\n        ])\n</code></pre> <ul> <li>Extract Themes: Look for recurring themes or topics within the interactions. This thematic analysis can help anticipate user interests or questions even before they are explicitly stated.</li> </ul> retrieve from memory stream<pre><code>def get_memory(self) -&gt; list[MemoryItem]:\n        return self.memory\n</code></pre>"},{"location":"concepts/#entity-knowledge-store","title":"Entity Knowledge Store","text":"<p>The Entity Knowledge Store tracks the frequency and recency of references to each entity stored in the memory stream. This knowledge store reflects users' depth of knowledge, i.e., concepts they are more familiar with than others. - Rank Entities by Relevance: Use both frequency and recency to rank entities. An entity frequently mentioned (high count) and referenced recently is likely of high importance, and the user is well aware of this concept.</p> select most relevant entities<pre><code>def _select_top_entities(self):\n        entity_knowledge_store = self.message.llm_message['knowledge_entity_store']\n        entities = [entity.to_dict() for entity in entity_knowledge_store]\n        entity_counts = [entity['count'] for entity in entities]\n        top_indexes = np.argsort(entity_counts)[:TOP_ENTITIES]\n        return [entities[index] for index in top_indexes]\n</code></pre> <ul> <li>Categorize Entities: Group entities into categories based on their nature or the context in which they're mentioned (e.g., technical terms, personal interests). This categorization aids in quickly accessing relevant information tailored to the user's inquiries.</li> </ul> group entities<pre><code>def _convert_memory_to_knowledge_memory(\n            self, memory_stream: list) -&gt; list[KnowledgeMemoryItem]:\n        \"\"\"Converts memory from memory stream to entity knowledge store by grouping entities \n\n        Returns:\n            knowledge_memory (list): list of KnowledgeMemoryItem\n        \"\"\"\n        knowledge_memory = []\n\n        entities = set([item.entity for item in memory_stream])\n        for entity in entities:\n            memory_dates = [\n                item.date for item in memory_stream if item.entity == entity\n            ]\n            knowledge_memory.append(\n                KnowledgeMemoryItem(entity, len(memory_dates),\n                                    max(memory_dates)))\n        return knowledge_memory\n</code></pre> <ul> <li>Highlight Changes Over Time: Identify any significant changes in the entities' ranking or categorization over time. A shift in the most frequently mentioned entities could indicate a change in the user's interests or knowledge.</li> <li>Additional information on the memory module can be found here</li> </ul> <p>Purpose in larger system</p> <ul> <li>Compress/summarize the top N ranked entities in the entity knowledge store and pass to the LLM\u2019s finite context window alongside the agent's response and chat history for inference.</li> <li>Personalize Responses: Use the key categorized entities and themes associated with the user to tailor agent responses more closely to the user's current interests and knowledge level/expertise.</li> <li>Anticipate Needs: Leverage trends and shifts identified in the summaries to anticipate users' future questions or needs.</li> </ul> <p>Future Contributions</p> <p>We currently extract the top N entities from the entitiy knowledge store and pass these entities into the context window for inference. memary can future benefit from more advanced memory compression techniques such as passing only entities that are in the agent's response to the context window. We look forward to related community contributions.</p> <p></p>"},{"location":"concepts/#new-context-window","title":"New Context Window","text":"<p>Note</p> <p>We utilize the the key categorized entities and themes associated with users to tailor agent responses more closely to the user's current interests/preferences and knowledge level/expertise. The new context window is made up of the following: </p> <ul> <li> <p>Agent response  retrieve agent response<pre><code>def get_routing_agent_response(self, query, return_entity=False):\n        \"\"\"Get response from the ReAct.\"\"\"\n        response = \"\"\n        if self.debug:\n            # writes ReAct agent steps to separate file and modifies format to be readable in .txt file\n            with open(\"data/routing_response.txt\", \"w\") as f:\n                orig_stdout = sys.stdout\n                sys.stdout = f\n                response = str(self.query(query))\n                sys.stdout.flush()\n                sys.stdout = orig_stdout\n            text = \"\"\n            with open(\"data/routing_response.txt\", \"r\") as f:\n                text = f.read()\n\n            plain = ansi_strip(text)\n            with open(\"data/routing_response.txt\", \"w\") as f:\n                f.write(plain)\n        else:\n            response = str(self.query(query))\n\n        if return_entity:\n            # the query above already adds final response to KG so entities will be present in the KG\n            return response, self.get_entity(self.query_engine.retrieve(query))\n        return response\n</code></pre></p> </li> <li> <p>Most relevant entities  retrieve important entities<pre><code>def get_entity(self, retrieve) -&gt; list[str]:\n        \"\"\"retrieve is a list of QueryBundle objects.\n        A retrieved QueryBundle object has a \"node\" attribute,\n        which has a \"metadata\" attribute.\n\n        example for \"kg_rel_map\":\n        kg_rel_map = {\n            'Harry': [['DREAMED_OF', 'Unknown relation'], ['FELL_HARD_ON', 'Concrete floor']],\n            'Potter': [['WORE', 'Round glasses'], ['HAD', 'Dream']]\n        }\n\n        Args:\n            retrieve (list[NodeWithScore]): list of NodeWithScore objects\n        return:\n            list[str]: list of string entities\n        \"\"\"\n\n        entities = []\n        kg_rel_map = retrieve[0].node.metadata[\"kg_rel_map\"]\n        for key, items in kg_rel_map.items():\n            # key is the entity of question\n            entities.append(key)\n            # items is a list of [relationship, entity]\n            entities.extend(item[1] for item in items)\n            if len(entities) &gt; MAX_ENTITIES_FROM_KG:\n                break\n        entities = list(set(entities))\n        for exceptions in ENTITY_EXCEPTIONS:\n            if exceptions in entities:\n                entities.remove(exceptions)\n        return entities\n</code></pre></p> </li> <li> <p>Chat history (summarized to avoid token overflow) summarize chat history<pre><code>def _summarize_contexts(self, total_tokens: int):\n        \"\"\"Summarize the contexts.\n\n        Args:\n            total_tokens (int): total tokens in the response\n        \"\"\"\n        messages = self.message.llm_message[\"messages\"]\n\n        # First two messages are system and user personas\n        if len(messages) &gt; 2 + NONEVICTION_LENGTH:\n            messages = messages[2:-NONEVICTION_LENGTH]\n            del self.message.llm_message[\"messages\"][2:-NONEVICTION_LENGTH]\n        else:\n            messages = messages[2:]\n            del self.message.llm_message[\"messages\"][2:]\n\n        message_contents = [message.to_dict()[\"content\"] for message in messages]\n\n        llm_message_chatgpt = {\n            \"model\": self.model,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Summarize these previous conversations into 50 words:\"\n                    + str(message_contents),\n                }\n            ],\n        }\n        response, _ = self._get_gpt_response(llm_message_chatgpt)\n        content = \"Summarized past conversation:\" + response\n        self._add_contexts_to_llm_message(\"assistant\", content, index=2)\n        logging.info(f\"Contexts summarized successfully. \\n summary: {response}\")\n        logging.info(f\"Total tokens after eviction: {total_tokens*EVICTION_RATE}\")\n</code></pre></p> </li> </ul>"},{"location":"contribute/","title":"Future Integrations","text":"<p>As mentioned, memary will benefit from the following integrations:</p> <ul> <li>Create an LLM Judge that scores the ReACT agent forming a feedback loop. See Zooter for insights.</li> <li>Expand the knowledge graph\u2019s capabilities to support multiple modalities, i.e., images.</li> <li>Optimize the graph to reduce latency of search times.</li> <li>Instead of extracting the top N entities from the entity knowledge store deploy more advanced memory compression techniques such as extracting only the entities included in the agent\u2019s response.</li> </ul> <p>memary's Future Structure</p> <p>Currently memary is structured so that the ReAct agent can only process one query at a time. We hope to see multiprocessing integrated so that the agent can process many subqueries simultaneously. We expect this to improve the relevancy and accuracy of responses. The source code for both decomposing the query and reranking the many agent responses has been provided, and once multiprocessing has been added to the system, these components can easily be integrated into the main <code>ChatAgent</code> class. The diagram below shows how the newly integrated system would work.</p> <p> </p>"},{"location":"contribute/#query-decomposition","title":"Query Decomposition","text":""},{"location":"contribute/#why-decompose","title":"Why decompose?","text":"<ul> <li>User queries are complex and multifaceted, and base-model LLMs are often unable to fully understand all aspects of the query in order to create a succinct and accurate response.</li> <li>Allows an LLM of similar capabilities to answer easier questions and synthesize those answers to provide an improved response.</li> </ul>"},{"location":"contribute/#how-it-works","title":"How it works","text":"<ul> <li> <p>Initially, a LlamaIndex fine-tuned query engine approach was taken. However, the LangChain query engine was found to be faster and easier to use. LangChain\u2019s PydanticToolsParser framework was used. The query_engine_with_examples has been given 87 pre-decomposed queries (complex query + set of subqueries) to determine a pattern. Users can invoke the engine with individual queries or collate them into a list and invoke them by batch.</p> </li> <li> <p>Individual Invocation:  Individual decomposition<pre><code>sub_qs = query_analyzer_with_examples.invoke( {\"question\": \"What is 2 + 2? Why is it not 3?\"} )\n</code></pre></p> </li> <li> <p>Batch Invocation: Batch decomposition<pre><code>questions = [\"Where can I buy a Macbook Pro with an M3 chip? What is the difference to the M2 chip? How much more expensive is  the M3?\", \"How can I buy tickets to the upcoming NBA game? What is the price of lower bowl seats versus nosebleeds? What is the view like at either seat?\", \"Between a Macbook and a Windows machine, which is better for systems engineering? Which chips are most ideal? What is the price difference between the two?\",] \n\nresponses = [] for question in questions: \nresponses.append(query_analyzer_with_examples.invoke({\"question\": question}))\n</code></pre></p> </li> </ul> <p>Purpose in larger system</p> <ul> <li>In a parallel system, the agent will be able to parse multiple queries at once. The query decomposer (QD) will pass all subqueries (or original query if no subqueries exist) to the agent at once.</li> <li>Simultaneously, QD will pass the original query to the reranking module to rerank the agent responses based on their relevance to the pre-decomposed query.</li> </ul> <p>Future Contributions</p> <p>Self-Learning: Whenever queries are decomposed, those examples will be appended to the engine\u2019s example store as a feedback loop for improved future performance.</p>"},{"location":"contribute/#reranking","title":"Reranking","text":""},{"location":"contribute/#why-rerank-responses","title":"Why rerank responses?","text":"<p>Ensure that the various responses to subqueries, when merged, are relevant to the original query prior to decomposition.</p>"},{"location":"contribute/#our-approach","title":"Our approach","text":"<ul> <li>We benchmarked three models to determine which one would best work for reranking: BM25 Reranking Fusion, Cohere Rerank, and ColBERT Rerank. After testing BM25, it was clear that the model was not able to classify the different responses and provide a simple merged answer. Instead of answering the question, it combined all the information on the page, introducing irrelevant information.</li> <li>Next, when testing out Cohere, the model performed better than BM25 but was still not classifying the paragraphs well. The reranking was not always accurate, as it performed well for some questions but was not able to rank others. Furthermore, the ranking was still pretty inaccurate, performing between 0.25 - 0.5 out of 1.</li> <li>Finally, we tested ColBERT rerank, and it was found that this model performed best compared to the other two. ColBERT was able to synthesize results from the given data and ranked them very accurately, with reranking scores between 0.6 - 0.7 out of 1. With this, ColBERT had the most potential, being able to determine which responses were most related and important to answering the query.</li> </ul> <p>Purpose in larger system</p> <p>Passes the reranking result to the knowledge graph for storage and to the new context window as one source of context for inference.</p> <p>Future Contributions</p> <p>Future Benchmarking: Include the Cohere Rerank 3 model and others in the reranking analysis. The data used for benchmarking can be found here. Add to it!</p>"},{"location":"evaluation/","title":"System Benchmarking","text":"<p>COMING SOON</p>"},{"location":"updates/","title":"Updates","text":"<p>1,000+ Repo Stars</p> <p>06/02/2024 </p> <ul> <li> <p>memary hits 1,000+ repo </p> </li> <li> <p>Achieved in 37 days. </p> </li> <li> <p>Announcement here.</p> </li> </ul> <p>Austin AI Meetup</p> <p>05/28/2024</p> <ul> <li>We presented memary at Austin AI's monthly meetup! </li> </ul> <p></p> <p></p> <p>v0.1.3 Release</p> <p>05/27/2024 </p> <ul> <li> <p>llama 3 8B/40B &amp; LLaVA suggested as default models.</p> </li> <li> <p>memary is now a foundational memory unit and can support any agent configurations by easily adding/removing tools.</p> </li> <li> <p>Watch our release demo here.</p> </li> </ul> <p>Llama Index Webinar</p> <p>05/23/2024 </p> <ul> <li>memary presented at Llama Index's webinar.</li> </ul> <p>v0.1.2 Release</p> <p>05/20/2024 </p> <ul> <li> <p>Ollama integration: option to switch between any downloaded model.</p> </li> <li> <p>llama3 8B and llava as default models.</p> </li> </ul> <p>v0.1.1 Release</p> <p>05/10/2024 </p> <ul> <li> <p>COLBERT reranking </p> </li> <li> <p>Query decomposition </p> </li> <li> <p>PyPi integration: <code>pip install memary</code></p> </li> <li> <p>Option to switch between tools </p> </li> </ul> <p>Mentioned in Geek News</p> <p>05/01/2024 </p> <ul> <li>Read the discussion here.</li> </ul> <p>Trending #16 on YC Hacker News</p> <p>04/29/2024 </p> <ul> <li> <p>16 on YC Hacker News. </p> </li> <li> <p>Read the discussion here. </p> </li> </ul> <p>Mentioned in AI Advances</p> <p>04/29/2024 </p> <ul> <li> <p>First community write up on memary! </p> </li> <li> <p>Read the overview here. </p> </li> </ul>"}]}